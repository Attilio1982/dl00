{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regolarizzazione L1 e L2\n",
    "\n",
    "La regolarizzazione è una tecnica utilizzata per contrastare il problema dell'overfitting, una situazione che si manifesta quando la rete memorizza le caratteristiche del set di addestramento piuttosto che apprendere da esse.\n",
    "<br>\n",
    "<br>\n",
    "L'overfitting è caratterizzato da:\n",
    "\n",
    "- **Alta variaza:** le previsioni per modelli addestrati con diverse parti del dataset saranno molto diverse tra loro.\n",
    "- **Basso bias:** l'errore per le predizioni sul set di addestramento è mediamente molto basso.\n",
    "\n",
    "<img src=\"res/overfitting.png\" width=\"500px\" /><br>\n",
    "\n",
    "In questo notebook vedremo come contrastare l'overfitting della nostra rete addestrata sul Fashion MNIST utilizzando la regolarizzazione L1 e L2.\n",
    "<br>\n",
    "Importiamo i moduli che ci serviranno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.callbacks import History \n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from time import time\n",
    "\n",
    "from scripts.random import set_seed # da utilizzare per poter riprodurre i miei risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparazione dei dati\n",
    "Carichiamo il dataset Fashion MNIST utilizzando Keras e preprocessiamo i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# Caricamento del dataset\n",
    "\n",
    "labels = [\"T-shirt/top\",\"Pantalone\",\"Pullover\",\"Vestito\",\"Cappotto\",\"Sandalo\",\"Maglietta\",\"Sneaker\",\"Borsa\",\"Stivaletto\"]\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Encoding delle immagini\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28*28)\n",
    "X_test = X_test.reshape(X_test.shape[0],28*28)\n",
    "\n",
    "# Normalizzazione\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "# Encoding del target\n",
    "\n",
    "num_classes=10\n",
    "\n",
    "y_train_dummy = to_categorical(y_train, num_classes)\n",
    "y_test_dummy = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso ogni immagine è codificata in un vettore contenente il valore dei pixel normalizzati disposti su di un'unica riga.\n",
    "<br>\n",
    "Il target è codificato all'interno di 10 variabili dummy, una per ogni classe, in cui la variabile alla posizione della classe di appartenenza vale 1 (True), mentre le altre valgono 0 (False)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riconoscere l'overfitting\n",
    "Evidenziare un problema di overfitting è molto semplice, un modello che ne soffre avrà memorizzato la struttura dei dati di addestramento, piuttosto che imparare da essi, quindi l'errore per le predizioni sul train set sarà molto basso, invece fallirà nel generalizzare, perciò l'errore nel test set sarà decisamente più alto.<br><br>\n",
    "Quindi per riconoscere l'overfitting è sufficente confrontare questi due valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "environ[\"PYTHONHASHSEED\"] = '0'\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]='-1'\n",
    "environ[\"TF_CUDNN_USE_AUTOTUNE\"] ='0'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "     \n",
    "from numpy.random import seed as np_seed\n",
    "np_seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "tf.set_random_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1_l2, l2\n",
    "\n",
    "set_seed(0) # per poter riprodurre i miei risultati\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam()\n",
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.5726 - acc: 0.7979\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3838 - acc: 0.8624\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.3371 - acc: 0.8787\n",
      "Train Accuracy = 0.8847 - Train Loss = 0.3133\n",
      "Test Accuracy = 0.8650 - Test Loss = 0.3728\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Epoch 3/3\n",
    "60000/60000 [==============================] - 2s 35us/step - loss: 0.3373 - acc: 0.8782\n",
    "Train Accuracy = 0.8842 - Train Loss = 0.3160\n",
    "Test Accuracy = 0.8642 - Test Loss = 0.3749\n",
    "\"\"\"\n",
    "\n",
    "model.fit(X_train, y_train_dummy, epochs=3, batch_size=512)\n",
    "    \n",
    "metrics_train = model.evaluate(X_train, y_train_dummy, verbose=0)\n",
    "metrics_test = model.evaluate(X_test, y_test_dummy, verbose=0)\n",
    "\n",
    "print(\"Train Accuracy = %.4f - Train Loss = %.4f\" % (metrics_train[1], metrics_train[0]))\n",
    "print(\"Test Accuracy = %.4f - Test Loss = %.4f\" % (metrics_test[1], metrics_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La differenza tra errore sul set di addestramento e sul set di test è elevatissima, specialmente per quanto riguarda la log loss. Siamo di fronte a un caso di overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizzare la regolarizzazione con Keras\n",
    "\n",
    "Con Keras possiamo regolarizzare pesi, bias e output lineare di un determinato strato utilizzando i seguenti parametri della classe Dense:\n",
    "\n",
    "- **kernel_regularizer**: applica la regolarizzazione ai pesi di uno strato.\n",
    "- **bias_regularizer**: applica la regolarizzazione ai bias di uno strato.\n",
    "- **activity_regularizer**: applica la regolarizzazione all'ouput lineare di uno strato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applicare la regolarizzazione L2\n",
    "\n",
    "La **regolarizzazione L2** è una tecnica di regolarizzazione che consiste nell'aggiungere una penalità per i pesi nella funzione di costo durante la fase di addestramento.<br>\n",
    "La penalità è data dalla somma dei quadrati dei pesi:\n",
    "$$\\lambda\\sum_{j=1}^{M}W_j^2$$<br>\n",
    "**Lambda** è il **parametro di regolarizzazione** ed è un'altro iperparametro.\n",
    "<br><br>\n",
    "Possiamo utilizzare la regolarizzazione L2 in uno strato della nostra passando al parametro la funzione l2 con all'interno il valore del parametro di regolarizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "set_seed(0) # per poter riprodurre i miei risultati\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1], kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train_dummy, epochs=100, batch_size=512)\n",
    "    \n",
    "metrics_train = model.evaluate(X_train, y_train_dummy, verbose=0)\n",
    "metrics_test = model.evaluate(X_test, y_test_dummy, verbose=0)\n",
    "\n",
    "print(\"Train Accuracy = %.4f - Train Loss = %.4f\" % (metrics_train[1], metrics_train[0]))\n",
    "print(\"Test Accuracy = %.4f - Test Loss = %.4f\" % (metrics_test[1], metrics_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggiungere la regolarizzazione L1\n",
    "\n",
    "La regolarizzazione L1 è un'altra tecnica di regolarizzazione che funziona in maniera simile alla L2, con la differenza che il termine di regolarizza sarà dato dalla somma del valore assoluto dei pesi:\n",
    "$$\\lambda\\sum_{j=1}^{M}|W_j|$$<br>\n",
    "e viene sempre applicato alla funzione di costo durante la fase di addestramento. La regolarizzazione L1 è molto più intensa della L2 e tende a ridurre a 0 molti pesi.\n",
    "<br>\n",
    "Nella pratica la regolarizzazione L2 porta quasi sempre a migliori risultati della L1, una buona tecnica consiste nell'utilizzarle insieme, con Keras possiamo farlo passando al parametro kernel_regularizer la funzione l1_l2 (se volessi utilizzare solo la L1, puoi anche farlo utilizzando la funzione l1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missinglink\n",
    "missinglink_callback = missinglink.KerasCallback(project='5721485469548544')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.regularizers import l1 # Nel caso volessi utilizzare solo la regolarizzazione l1\n",
    "from keras.regularizers import l1_l2,l2\n",
    "\n",
    "\"\"\"\n",
    "Train Accuracy = 0.8295 - Train Loss = 0.7705\n",
    "Test Accuracy = 0.8204 - Test Loss = 0.8011\n",
    "\"\"\"\n",
    "\n",
    "set_seed(0) # per poter riprodurre i miei risultati\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1], kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=0.0001,l2=0.001)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train_dummy, epochs=3, batch_size=512, callbacks=[missinglink_callback])\n",
    "\n",
    "metrics_train = model.evaluate(X_train, y_train_dummy, verbose=0)\n",
    "metrics_test = model.evaluate(X_test, y_test_dummy, verbose=0)\n",
    "\n",
    "print(\"Train Accuracy = %.4f - Train Loss = %.4f\" % (metrics_train[1], metrics_train[0]))\n",
    "print(\"Test Accuracy = %.4f - Test Loss = %.4f\" % (metrics_test[1], metrics_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamo ancora dai risultati ottimali, ma applicando la regolarizzazione siamo riusciti a contrastare l'overfitting e abbiamo creato un modello in grado di apprendere dai dati di addestramento e generalizzare su dati sconosciuti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM con Keras.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "mNsRrhnKLCdu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM con Keras\n",
        "\n",
        "Le reti **Long short-term memory (LSTM)** sono un'architettura di reti neurali ricorrenti che risolvono il problema della scomparsa del gradiente tra le diverse esecuzioni della rete.\n",
        "<br>\n",
        "Le LSTM  sono state da Sepp Hochreiter e Jurger Schmidhuber nel 1997 in [questa ricerca](https://www.bioinf.jku.at/publications/older/2604.pdf) ed in sostanza aggiungono uno stato alla rete (cell state) che viaggia in parallelo al segnale della rete e immagazzina le informazioni sequenziali.\n",
        "<br><br>\n",
        "In questo notebook utilizzeremo le LSTM per migliorare la rete neurale per la sentiment analysis addestrata sull'IMDB Movie Reviews dataset."
      ]
    },
    {
      "metadata": {
        "id": "ZrXkW7gvK2BY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fTpJ1pTBLHMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Scarichiamo il dataset\n",
        "Utilizziamo Keras per caricare l'imdb dataset, limitandolo alle 10000 parole più comuni."
      ]
    },
    {
      "metadata": {
        "id": "UPH0ozUGLIqF",
        "colab_type": "code",
        "outputId": "ad2f920f-4407-4688-a7c5-9867735b2f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb \n",
        "\n",
        "num_words = 10000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "print(\"Numero di esempi nel train set: %d\" % len(X_train))\n",
        "print(\"Numero di esempi nel test set: %d\" % len(X_test))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero di esempi nel train set: 25000\n",
            "Numero di esempi nel test set: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "79O1jbZyLZ5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessiamo i dati\n",
        "Le recensioni all'interno del corpus di testo hanno ovviamente lunghezza differente, utilizziamo la funzione pad_sequences di keras per limitare le sequenze a 500 elementi (nel nostro caso limitare le frasi a 500 parole).\n",
        "Se una sequenza ha meno di 500 esempi verranno aggiunti degli zeri alla fine.\n"
      ]
    },
    {
      "metadata": {
        "id": "bUH2iqw6LI65",
        "colab_type": "code",
        "outputId": "1a09fb59-0225-4069-ae8d-872e8c560e23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 500\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen = maxlen)\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "C6wbxGWnLfeO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creiamo il modello\n",
        "Costruiamo il nostro modello strutturandolo così:\n",
        "1. Il primo strato eseguirà l'embedding creando 50 embedding vectors per ognuna delle 10.000 parole nel nostro dizionario.\n",
        "2. Il secondo strato è lo strato ricorrente di tipo Long-short term memory.\n",
        "3. Il terzo strato calcolerà l'ouput della rete, trattandosi di un problema di classifcazione binaria (recensione positiva/negativa) la funzione di attivazione sarà la sigmoide."
      ]
    },
    {
      "metadata": {
        "id": "A5ETI402Ldz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "1b3ee0fe-b375-4088-8959-c33514ba3b93"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 50))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 50)          500000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                10624     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 510,657\n",
            "Trainable params: 510,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mDmkp4hCyAmo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiliamo il modello ed eseguiamo l'addestramento per 5 epoche."
      ]
    },
    {
      "metadata": {
        "id": "dlKVR0tQL4az",
        "colab_type": "code",
        "outputId": "fe49bee4-3178-4ee8-88cb-cd6f1ce1d598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 37s 2ms/step - loss: 0.6412 - acc: 0.6766 - val_loss: 0.5538 - val_acc: 0.7566\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 34s 2ms/step - loss: 0.4363 - acc: 0.8274 - val_loss: 0.3925 - val_acc: 0.8382\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 34s 2ms/step - loss: 0.3244 - acc: 0.8763 - val_loss: 0.4741 - val_acc: 0.8024\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 34s 2ms/step - loss: 0.2708 - acc: 0.8995 - val_loss: 0.3731 - val_acc: 0.8464\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 34s 2ms/step - loss: 0.2321 - acc: 0.9136 - val_loss: 0.3100 - val_acc: 0.8806\n",
            "25000/25000 [==============================] - 119s 5ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.32797487154483795, 0.87216]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "CBufe-oDyKdU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Il risultato è nettamente migliore rispetto ad una semplice RNN, c'è un po' di overfitting, proviamo a correggerlo utilizzando il dropout."
      ]
    },
    {
      "metadata": {
        "id": "-zc-XPzJMeqJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dropout in una RNN\n",
        "In una RNN possiamo utilizzare due differenti tipologie di dropout.\n",
        "1. Dropout sull'input/output dello strato, esattamente come abbiamo già fatto con le altre architetture di reti neurali.\n",
        "2. Dropout tra le esecuzioni ricorrenti della rete, questo permette di ridurre l'overfitting nelle features che contengono le informazioni sulla sequenza.\n",
        "\n",
        "Per utilizzare il dropout sull'input di uno strato ricorrente, piuttosto che usare la classe Dropout, è consigliato sfruttare il parametro dropout delle classi SimpleRNN e LSTM.\n",
        "Per utilizzare il dropout ricorrente possiamo invece utilizzare il parametro recurrent_dropout delle classi SimpleRNN e LSTM."
      ]
    },
    {
      "metadata": {
        "id": "g4U8by-MMi3l",
        "colab_type": "code",
        "outputId": "a1d306d9-9f3e-414d-a979-389ba86b0093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, LSTM, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 50))\n",
        "model.add(LSTM(32, dropout=0.4, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 50)          500000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                10624     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 510,657\n",
            "Trainable params: 510,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l0HQyS2bzfSr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiliamo il modello ed eseguiamo l'addestramento per 10 epoche."
      ]
    },
    {
      "metadata": {
        "id": "ve-NT35KL6D6",
        "colab_type": "code",
        "outputId": "6d8e7d22-d480-40cf-d42b-0c0dacd53cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 41s 2ms/step - loss: 0.6590 - acc: 0.6369 - val_loss: 0.5685 - val_acc: 0.7596\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 40s 2ms/step - loss: 0.4999 - acc: 0.8003 - val_loss: 0.4337 - val_acc: 0.8184\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 40s 2ms/step - loss: 0.4029 - acc: 0.8409 - val_loss: 0.3740 - val_acc: 0.8418\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 40s 2ms/step - loss: 0.3614 - acc: 0.8567 - val_loss: 0.3482 - val_acc: 0.8534\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 40s 2ms/step - loss: 0.3276 - acc: 0.8740 - val_loss: 0.4137 - val_acc: 0.8126\n",
            "25000/25000 [==============================] - 139s 6ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4099230393791199, 0.81852]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "bAIF5qU_zegw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Abbiamo ridotto l'overfitting nella nostra rete, adesso proviamo a migliorare il modello aggiungendo degli altri strati ricorrenti alla rete."
      ]
    },
    {
      "metadata": {
        "id": "_SNgyO0XMtUR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Aggiungiamo altri strati LSTM\n",
        "Possiamo aggiungere altri strati ricorrenti ad una rete nella solita maniera, utilizzando il metodo add della classe Sequential.\n",
        "Tieni conto solo di una cosa, di default la classe LSTM esegue il flattening della sequenza per poterla dare come input ad uno strato denso, possiamo modificare tale comportamento impostando il parametro return_sequences a True."
      ]
    },
    {
      "metadata": {
        "id": "KYVykzX3MvgK",
        "colab_type": "code",
        "outputId": "678cb208-f980-4e7d-9253-6b9bfca93f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, LSTM, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 100))\n",
        "model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, None, 32)          17024     \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,025,377\n",
            "Trainable params: 1,025,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tk22Y0290lBE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiliamo il modello ed eseguiamo l'addestramento per 10 epoche."
      ]
    },
    {
      "metadata": {
        "id": "t3AkxMtXM9tt",
        "colab_type": "code",
        "outputId": "aef6090f-29df-4c69-bbd7-cebc86f6b10b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 85s 4ms/step - loss: 0.6192 - acc: 0.6643 - val_loss: 0.5286 - val_acc: 0.7272\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 82s 4ms/step - loss: 0.4371 - acc: 0.8109 - val_loss: 0.4941 - val_acc: 0.7902\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 82s 4ms/step - loss: 0.3732 - acc: 0.8506 - val_loss: 0.3938 - val_acc: 0.8306\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 82s 4ms/step - loss: 0.3429 - acc: 0.8667 - val_loss: 0.3781 - val_acc: 0.8352\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 82s 4ms/step - loss: 0.3077 - acc: 0.8846 - val_loss: 0.3732 - val_acc: 0.8370\n",
            "25000/25000 [==============================] - 275s 11ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3764999796962738, 0.8376]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "bUJs-j1-bRT2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN + LSTM"
      ]
    },
    {
      "metadata": {
        "id": "_h58JpCkbg0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "4f0382e6-2fc5-465e-d8ba-bd37b9af6084"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 50))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(32, dropout=0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, None, 50)          500000    \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, None, 32)          4832      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, None, 32)          0         \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 513,185\n",
            "Trainable params: 513,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T1kICKBCXcEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "0de879a3-1ad7-4949-f135-5d4548d994c4"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 23s 1ms/step - loss: 0.6243 - acc: 0.6450 - val_loss: 0.4240 - val_acc: 0.8290\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 20s 1ms/step - loss: 0.4186 - acc: 0.8144 - val_loss: 0.3573 - val_acc: 0.8614\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 20s 1ms/step - loss: 0.3249 - acc: 0.8629 - val_loss: 0.3128 - val_acc: 0.8726\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 20s 1ms/step - loss: 0.2735 - acc: 0.8909 - val_loss: 0.3690 - val_acc: 0.8562\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 20s 1ms/step - loss: 0.2443 - acc: 0.9039 - val_loss: 0.2809 - val_acc: 0.8824\n",
            "25000/25000 [==============================] - 66s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2899127991676331, 0.87956]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}
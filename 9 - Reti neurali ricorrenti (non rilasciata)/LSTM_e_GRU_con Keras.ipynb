{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM con Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfAI/dl00/blob/master/9%20-%20Reti%20neurali%20ricorrenti%20(non%20rilasciata)/LSTM_e_GRU_con%20Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mNsRrhnKLCdu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM e GRU con Keras\n",
        "\n",
        "Le reti **Long short-term memory (LSTM)** sono un'architettura di reti neurali ricorrenti che risolvono il problema della scomparsa del gradiente tra le diverse esecuzioni della rete.\n",
        "<br>\n",
        "Le LSTM  sono state da Sepp Hochreiter e Jurger Schmidhuber nel 1997 in [questa ricerca](https://www.bioinf.jku.at/publications/older/2604.pdf) ed in sostanza aggiungono canale prioritario alla rete, chiamato cell state o memory cell, che viaggia in parallelo al segnale della rete e immagazzina le informazioni sequenziali.\n",
        "<br><br>\n",
        "In questo notebook utilizzeremo le LSTM per migliorare la rete neurale per la sentiment analysis addestrata sull'IMDB Movie Reviews dataset."
      ]
    },
    {
      "metadata": {
        "id": "ZrXkW7gvK2BY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "851ba194-4337-41ab-9794-f1d8c0801e21"
      },
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fTpJ1pTBLHMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Scarichiamo il dataset\n",
        "Utilizziamo Keras per caricare l'imdb dataset, limitandolo alle 10000 parole più comuni."
      ]
    },
    {
      "metadata": {
        "id": "UPH0ozUGLIqF",
        "colab_type": "code",
        "outputId": "74058ae5-18c2-4f3e-e632-73015baa8052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb \n",
        "\n",
        "num_words = 10000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "print(\"Numero di esempi nel train set: %d\" % len(X_train))\n",
        "print(\"Numero di esempi nel test set: %d\" % len(X_test))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "Numero di esempi nel train set: 25000\n",
            "Numero di esempi nel test set: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "79O1jbZyLZ5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessiamo i dati\n",
        "Le recensioni all'interno del corpus di testo hanno ovviamente lunghezza differente, utilizziamo la funzione pad_sequences di keras per limitare le sequenze a 500 elementi (nel nostro caso limitare le frasi a 500 parole).\n",
        "Se una sequenza ha meno di 500 esempi verranno aggiunti degli zeri alla fine.\n"
      ]
    },
    {
      "metadata": {
        "id": "bUH2iqw6LI65",
        "colab_type": "code",
        "outputId": "c3fc338b-b5bc-4f81-857b-224d6dd4c1ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 500\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen = maxlen)\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "C6wbxGWnLfeO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creiamo il modello LSTM\n",
        "Costruiamo il nostro modello strutturandolo così:\n",
        "1. Il primo strato eseguirà l'embedding creando 50 embedding vectors per ognuna delle 10.000 parole nel nostro dizionario.\n",
        "2. Il secondo strato è lo strato ricorrente di tipo Long-short term memory.\n",
        "3. Il terzo strato calcolerà l'ouput della rete, trattandosi di un problema di classifcazione binaria (recensione positiva/negativa) la funzione di attivazione sarà la sigmoide."
      ]
    },
    {
      "metadata": {
        "id": "A5ETI402Ldz_",
        "colab_type": "code",
        "outputId": "0ca3ac7f-1db2-4766-f52a-8988bf82358c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 50))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 50)          500000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                10624     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 510,657\n",
            "Trainable params: 510,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mDmkp4hCyAmo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiliamo il modello ed eseguiamo l'addestramento per 5 epoche."
      ]
    },
    {
      "metadata": {
        "id": "dlKVR0tQL4az",
        "colab_type": "code",
        "outputId": "fe49bee4-3178-4ee8-88cb-cd6f1ce1d598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 37s 2ms/step - loss: 0.6412 - acc: 0.6766 - val_loss: 0.5538 - val_acc: 0.7566\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 34s 2ms/step - loss: 0.4363 - acc: 0.8274 - val_loss: 0.3925 - val_acc: 0.8382\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 34s 2ms/step - loss: 0.3244 - acc: 0.8763 - val_loss: 0.4741 - val_acc: 0.8024\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 34s 2ms/step - loss: 0.2708 - acc: 0.8995 - val_loss: 0.3731 - val_acc: 0.8464\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 34s 2ms/step - loss: 0.2321 - acc: 0.9136 - val_loss: 0.3100 - val_acc: 0.8806\n",
            "25000/25000 [==============================] - 119s 5ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.32797487154483795, 0.87216]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "CBufe-oDyKdU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Il risultato è nettamente migliore rispetto ad una semplice RNN, c'è un po' di overfitting, proviamo a correggerlo utilizzando il dropout."
      ]
    },
    {
      "metadata": {
        "id": "-zc-XPzJMeqJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dropout in una RNN\n",
        "In una RNN possiamo utilizzare due differenti tipologie di dropout.\n",
        "1. Dropout sull'input/output dello strato, esattamente come abbiamo già fatto con le altre architetture di reti neurali.\n",
        "2. Dropout tra le esecuzioni ricorrenti della rete, questo permette di ridurre l'overfitting nelle features che contengono le informazioni sulla sequenza.\n",
        "\n",
        "Per utilizzare il dropout sull'input di uno strato ricorrente, piuttosto che usare la classe Dropout, è consigliato sfruttare il parametro dropout delle classi SimpleRNN e LSTM.\n",
        "Per utilizzare il dropout ricorrente possiamo invece utilizzare il parametro recurrent_dropout delle classi SimpleRNN e LSTM."
      ]
    },
    {
      "metadata": {
        "id": "g4U8by-MMi3l",
        "colab_type": "code",
        "outputId": "b406aad8-b016-42a7-8d79-5c2a160b2c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, LSTM, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 50))\n",
        "model.add(LSTM(32, dropout=0.4, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 50)          500000    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 32)                10624     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 510,657\n",
            "Trainable params: 510,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l0HQyS2bzfSr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiliamo il modello ed eseguiamo l'addestramento per 5 epoche."
      ]
    },
    {
      "metadata": {
        "id": "ve-NT35KL6D6",
        "colab_type": "code",
        "outputId": "6d8e7d22-d480-40cf-d42b-0c0dacd53cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 41s 2ms/step - loss: 0.6590 - acc: 0.6369 - val_loss: 0.5685 - val_acc: 0.7596\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 40s 2ms/step - loss: 0.4999 - acc: 0.8003 - val_loss: 0.4337 - val_acc: 0.8184\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 40s 2ms/step - loss: 0.4029 - acc: 0.8409 - val_loss: 0.3740 - val_acc: 0.8418\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 40s 2ms/step - loss: 0.3614 - acc: 0.8567 - val_loss: 0.3482 - val_acc: 0.8534\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 40s 2ms/step - loss: 0.3276 - acc: 0.8740 - val_loss: 0.4137 - val_acc: 0.8126\n",
            "25000/25000 [==============================] - 139s 6ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4099230393791199, 0.81852]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "bAIF5qU_zegw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Abbiamo ridotto l'overfitting nella nostra rete, adesso proviamo a migliorare il modello aggiungendo degli altri strati ricorrenti alla rete."
      ]
    },
    {
      "metadata": {
        "id": "_SNgyO0XMtUR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Aggiungiamo altri strati LSTM\n",
        "Possiamo aggiungere altri strati ricorrenti ad una rete nella solita maniera, utilizzando il metodo add della classe Sequential.\n",
        "Tieni conto solo di una cosa, di default la classe LSTM esegue il flattening della sequenza per poterla dare come input ad uno strato denso, possiamo modificare tale comportamento impostando il parametro return_sequences a True."
      ]
    },
    {
      "metadata": {
        "id": "KYVykzX3MvgK",
        "colab_type": "code",
        "outputId": "ef663f2a-6e58-49db-c54c-b548aec0db95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, LSTM, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 100))\n",
        "model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, None, 32)          17024     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,025,377\n",
            "Trainable params: 1,025,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tk22Y0290lBE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiliamo il modello ed eseguiamo l'addestramento per 5 epoche, questa volta cronometriamo la durata totale dell'addestramento, ci servirà dopo per eseguire il confronto con la variante GRU."
      ]
    },
    {
      "metadata": {
        "id": "t3AkxMtXM9tt",
        "colab_type": "code",
        "outputId": "a19d661c-d86d-4f77-ec81-6afa3e70d3e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "start_at = time()\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "print(\"Addestramento completato in %.f secondi (5 epoche)\" % ((time()-start_at)))\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 79s 4ms/step - loss: 0.6242 - acc: 0.6494 - val_loss: 0.6510 - val_acc: 0.7038\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 77s 4ms/step - loss: 0.4499 - acc: 0.8103 - val_loss: 0.3923 - val_acc: 0.8314\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 76s 4ms/step - loss: 0.3799 - acc: 0.8471 - val_loss: 0.4720 - val_acc: 0.7824\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 78s 4ms/step - loss: 0.3406 - acc: 0.8705 - val_loss: 0.3780 - val_acc: 0.8366\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 77s 4ms/step - loss: 0.3191 - acc: 0.8780 - val_loss: 0.3822 - val_acc: 0.8388\n",
            "Addestramento completato in 390 secondi (5 epoche)\n",
            "25000/25000 [==============================] - 275s 11ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3829597888946533, 0.83548]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "bUJs-j1-bRT2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gated Recurrent Unit (GRU)\n",
        "\n",
        "Le **Gated Recurrent Unit (GRU)** sono una tipologia di reti neurali ricorrenti che prendono spunto da e semplificano le LSTM.\n",
        "<br>\n",
        "A differenza di quest'ultime le GRU richiedono meno calcoli tensoriali e quindi solitamente portano a risultati simili in minor tempo.\n",
        "<br>\n",
        "Possiamo aggiungere degli strati GRU con Keras tramite la classe GRU, facciamolo utilizzando la stessa architettura della rete LSTM profonda."
      ]
    },
    {
      "metadata": {
        "id": "_h58JpCkbg0Y",
        "colab_type": "code",
        "outputId": "05134a5a-b3d3-4996-86e2-51bdab042639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, GRU, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 100))\n",
        "model.add(GRU(32, dropout=0.5, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(GRU(32, dropout=0.5, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, None, 32)          12768     \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 32)                6240      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,019,041\n",
            "Trainable params: 1,019,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0JxgtDBOYNc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiliamo il modello e avviamo l'addestramento per 5 epoche, cronometrandone la durata."
      ]
    },
    {
      "metadata": {
        "id": "T1kICKBCXcEn",
        "colab_type": "code",
        "outputId": "a1501d01-1967-436d-df37-37a9848a0811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "start_at = time()\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "print(\"Addestramento completato in %.f secondi (5 epoche)\" % ((time()-start_at)))\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 65s 3ms/step - loss: 0.6584 - acc: 0.6056 - val_loss: 0.5248 - val_acc: 0.7350\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 61s 3ms/step - loss: 0.4874 - acc: 0.7753 - val_loss: 0.4472 - val_acc: 0.7884\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 61s 3ms/step - loss: 0.4101 - acc: 0.8266 - val_loss: 0.4207 - val_acc: 0.8116\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 61s 3ms/step - loss: 0.3700 - acc: 0.8498 - val_loss: 0.4369 - val_acc: 0.8012\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 63s 3ms/step - loss: 0.3383 - acc: 0.8646 - val_loss: 0.4283 - val_acc: 0.8092\n",
            "Addestramento completato in 313 secondi (5 epoche)\n",
            "25000/25000 [==============================] - 211s 8ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4191904431438446, 0.8156]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "AdK6nkSLOkC3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Il risultato è livemente inferiore a quello ottenuto con le LSTM ma l'addestramento ha richiesto il 20% del tempo in meno."
      ]
    }
  ]
}